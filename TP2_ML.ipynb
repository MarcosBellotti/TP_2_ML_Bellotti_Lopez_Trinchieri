{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, Convolution2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuraciones iniciales de algunas constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIAS = 'buildings', 'forest', 'glacier', 'mountain', 'sea', 'street'\n",
    "\n",
    "TRAIN_DIR = Path('train')\n",
    "TEST_DIR = Path('test')\n",
    "SIZE = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de la calidad de las imágenes que serán utilizadas para entrenar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de comenzar a trabajar, revisamos en general las imagenes de train y nos encontramos con muchas imagenes que nada tenían que ver con la categoría a la que decian pertenecer, principalmente en **glacier**. Las mismas fueron removidas de dicho directorio y las guardamos en la carpeta deletes.\n",
    "\n",
    "Un ejemplo para que se comprenda el tipo de imágenes que quitamos, es el de una persona disfrazada de dinosaurio en el gran cañón categorizada como **glacier**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividimos las imagenes de train en dos dataset de train y validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desarrollamos una función que extrae el 20% de las imagenes de cada categoría para que pasen a ser imagenes de validation. Utilizamos el porcentaje y no una cantidad fija para mantener las proporciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "\n",
    "# def dividir_train_validation_existente(carpeta_train, carpeta_validation, porcentaje_validacion=0.2):\n",
    "\n",
    "#     if not os.path.exists(carpeta_validation):\n",
    "#         os.makedirs(carpeta_validation)\n",
    "\n",
    "#     for categoria in os.listdir(carpeta_train):\n",
    "#         ruta_categoria_train = os.path.join(carpeta_train, categoria)\n",
    "    \n",
    "#         if os.path.isdir(ruta_categoria_train):\n",
    "#             imagenes = [img for img in os.listdir(ruta_categoria_train) if os.path.isfile(os.path.join(ruta_categoria_train, img))]\n",
    "\n",
    "#             num_validacion = int(len(imagenes) * porcentaje_validacion)\n",
    "        \n",
    "#             ruta_categoria_validation = os.path.join(carpeta_validation, categoria)\n",
    "#             if not os.path.exists(ruta_categoria_validation):\n",
    "#                 os.makedirs(ruta_categoria_validation)\n",
    "\n",
    "#             for i in range(num_validacion):\n",
    "#                 imagen = imagenes[i]\n",
    "#                 ruta_imagen = os.path.join(ruta_categoria_train, imagen)\n",
    "#                 shutil.move(ruta_imagen, os.path.join(ruta_categoria_validation, imagen))\n",
    "\n",
    "#     print(\"División completada: 20% de las imágenes movidas a validation.\")\n",
    "\n",
    "# carpeta_train = 'train'\n",
    "# carpeta_validation = 'validation'\n",
    "\n",
    "# dividir_train_validation_existente(carpeta_train, carpeta_validation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis exploratorio del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def contar_imagenes_por_categoria(carpeta_train):\n",
    "    conteo_categorias = {}\n",
    "\n",
    "    for categoria in os.listdir(carpeta_train):\n",
    "        ruta_categoria = os.path.join(carpeta_train, categoria)\n",
    "\n",
    "        if os.path.isdir(ruta_categoria):\n",
    "            cantidad_imagenes = len([img for img in os.listdir(ruta_categoria) if os.path.isfile(os.path.join(ruta_categoria, img))])\n",
    "            conteo_categorias[categoria] = cantidad_imagenes\n",
    "\n",
    "    return conteo_categorias\n",
    "\n",
    "def graficar_distribucion(conteo_categorias):\n",
    "    categorias = list(conteo_categorias.keys())\n",
    "    cantidades = list(conteo_categorias.values())\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=categorias, y=cantidades, palette='viridis')\n",
    "\n",
    "\n",
    "    plt.title('Distribución de Imágenes por Categoría en la Carpeta Test', fontsize=14)\n",
    "    plt.xlabel('Categoría', fontsize=12)\n",
    "    plt.ylabel('Cantidad de Imágenes', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "conteo_categorias = contar_imagenes_por_categoria(TRAIN_DIR)\n",
    "\n",
    "graficar_distribucion(conteo_categorias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. *Volumetría de los datos y distribución de las variables a predecir:*\n",
    "   El conjunto de datos cuenta con un total aproximado de 14,000 imágenes, distribuidas en 6 categorías, de la siguiente manera:\n",
    "\n",
    "   - *Buildings*: 15.6%\n",
    "   - *Forest*: 16.2%\n",
    "   - *Glacier*: 17.1%\n",
    "   - *Mountain*: 17.9%\n",
    "   - *Sea*: 16.2%\n",
    "   - *Street*: 17.0%\n",
    "\n",
    "   Las categorías están relativamente balanceadas, con diferencias menores en el número de imágenes entre clases. Ninguna categoría domina el conjunto de datos, lo que favorece el entrenamiento de un modelo equilibrado. Esto implica que no sería necesario realizar ajustes significativos para balancear las clases en este punto.\n",
    "\n",
    "2. *Estructura y tipo de las imágenes:*\n",
    "   - Las imágenes tienen un tamaño de *150x150 píxeles*.\n",
    "   - El formato de las imágenes es JPG.\n",
    "   - Las imágenes pertenecen a paisajes y escenas específicas de categorías bien diferenciadas, como edificios, naturaleza, y calles.\n",
    "\n",
    "\n",
    "**Aclaración:** la distrución, luego de dividir train en dos sets nuevos, sigue siendo la misma proporción dado que nos quedamos con el 20% de cada categoría en validation y el 80% de cada categoría en train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Para comenzar, trabajamos con imágenes reescaladas, con el brillo cambiado y rotadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images_reader = ImageDataGenerator(\n",
    "#     rescale=1/255,\n",
    "#     rotation_range=10,\n",
    "#     brightness_range=(0.5, 1.5),\n",
    "#     # width_shift_range=0.3,\n",
    "#     # height_shift_range=0.3,\n",
    "#     # horizontal_flip=True,\n",
    "#     # fill_mode='nearest'    \n",
    "# )\n",
    "\n",
    "images_reader = ImageDataGenerator(\n",
    "    rescale=1/255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    brightness_range=(0.5, 1.5),\n",
    "    horizontal_flip=True,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    #vertical_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "  \n",
    "READ_PARAMS = dict(\n",
    "    class_mode=\"categorical\",\n",
    "    classes=CATEGORIAS,\n",
    "    target_size=(SIZE, SIZE),\n",
    "    color_mode=\"rgb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_DIR='validation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializamos los generadores para utilizar las imágenes durante el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = images_reader.flow_from_directory(TRAIN_DIR, **READ_PARAMS)\n",
    "validation = images_reader.flow_from_directory(VALIDATION_DIR, **READ_PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplos de imagenes a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(dataset):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    images, labels = next(dataset)\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i])\n",
    "        plt.title(CATEGORIAS[np.argmax(labels[i])])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_images(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_images(validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (SIZE, SIZE, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights_at_epochs = {}\n",
    "\n",
    "class OurCustomCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        model_weights_at_epochs[epoch] = self.model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP - Multi layer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la **arquitectura** del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelMLP = Sequential([\n",
    "    Input(input_shape),\n",
    "    \n",
    "    Flatten(),\n",
    "\n",
    "    Dense(500, activation='tanh'),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Dense(len(CATEGORIAS), activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelMLP.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "modelMLP.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el MLP con 5 epocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyMLP = modelMLP.fit(\n",
    "    train,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    validation_data=validation,\n",
    "    callbacks=[OurCustomCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graficamos la salida de la corrida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(historyMLP.history['accuracy'], label='train')\n",
    "plt.plot(historyMLP.history['val_accuracy'], label='validation')\n",
    "plt.title('Accuracy over train epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolucional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo convolucional con "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights_at_epochs = {}\n",
    "\n",
    "modelConvolucional = Sequential([\n",
    "    Input(input_shape),\n",
    "\n",
    "    Convolution2D(filters=10, kernel_size=(4, 4), strides=1, activation='relu'),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Convolution2D(filters=10, kernel_size=(4, 4), strides=1, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    MaxPooling2D(pool_size=(4, 4)),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(100, activation='tanh'),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Dense(len(CATEGORIAS), activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelConvolucional.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "modelConvolucional.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyConvolutional = modelConvolucional.fit(\n",
    "    train,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    validation_data=validation,\n",
    "    callbacks=[OurCustomCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(historyConvolutional.history['accuracy'], label='train')\n",
    "plt.plot(historyConvolutional.history['val_accuracy'], label='validation')\n",
    "plt.title('Accuracy over train epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolucional modificado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este modelo convolucional utiliza más filtros (32, 64 y 128) para capturar características detalladas en diferentes niveles de abstracción, mientras que los kernels más pequeños de 3x3 permiten una mejor eficiencia en la detección de patrones locales. Las capas de MaxPooling con un tamaño de 2x2 reducen el tamaño de las características progresivamente, ayudando a prevenir el sobreajuste. La activación relu en las capas densas mejora la eficiencia en la propagación de gradientes, y el Dropout más alto (0.5) en las últimas capas evita el sobreajuste, lo que hace al modelo más robusto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelConvolucionalV2 = Sequential([\n",
    "    Input(shape=(150, 150, 3)),\n",
    "\n",
    "    Convolution2D(filters=32, kernel_size=(3, 3), strides=1, activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Convolution2D(filters=64, kernel_size=(3, 3), strides=1, activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Convolution2D(filters=128, kernel_size=(3, 3), strides=1, activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    Dense(len(CATEGORIAS), activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelConvolucionalV2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "modelConvolucionalV2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyConvolutionalV2 = modelConvolucionalV2.fit(\n",
    "    train,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    validation_data=validation,\n",
    "    callbacks=[OurCustomCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(historyConvolutionalV2.history['accuracy'], label='train')\n",
    "plt.plot(historyConvolutionalV2.history['val_accuracy'], label='validation')\n",
    "plt.title('Accuracy over train epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolucional usando convoluciones ya entrenadas de VGG16\n",
    "pretrained_model = VGG16(input_shape=input_shape, include_top=False)\n",
    "pretrained_model.trainable = False\n",
    "\n",
    "modelVGG16 = Sequential([\n",
    "    pretrained_model,\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(100, activation='tanh'),\n",
    "    Dense(100, activation='tanh'),\n",
    "    \n",
    "    Dense(len(CATEGORIAS), activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import Xception\n",
    "\n",
    "\n",
    "# base_model = Xception(weights='imagenet', include_top=False, input_shape=(SIZE,SIZE,3))\n",
    "# base_model.trainable = True\n",
    "\n",
    "# for layer in base_model.layers[:100]:\n",
    "#     layer.trainable = False\n",
    "# for layer in base_model.layers[100:]:\n",
    "#     layer.trainable = True\n",
    "\n",
    "# modelVGG16 = Sequential([\n",
    "#     base_model,\n",
    "\n",
    "#     Flatten(),\n",
    "\n",
    "#     Dense(100, activation='tanh'),\n",
    "    \n",
    "#     Dense(len(CATEGORIAS), activation='softmax'),\n",
    "# ])\n",
    "# FUNCIONA - ver mas epocas\n",
    "# modelVGG16 = Sequential([\n",
    "#     base_model,\n",
    "\n",
    "#     Flatten(),\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dropout(0.5), \n",
    "#     Dense(128, activation='relu'),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(100, activation='relu'),\n",
    "#     Dense(len(CATEGORIAS), activation='softmax')\n",
    "\n",
    "# ])\n",
    "# FUNCIONA - ver mas epocas hasta 10\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "modelVGG16 = Sequential([\n",
    "    base_model,\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(256),\n",
    "    LeakyReLU(alpha=0.1),  \n",
    "    Dropout(0.25),\n",
    "    Dense(128),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    Dropout(0.25),\n",
    "    Dense(100),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    Dense(len(CATEGORIAS), activation='softmax')\n",
    "])\n",
    "\n",
    "# from tensorflow.keras.models import Model, Sequential\n",
    "# from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, LeakyReLU, Flatten\n",
    "# from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "# # Crear el modelo base pre-entrenado\n",
    "# base_model = Xception(weights='imagenet', include_top=False, input_shape=(SIZE, SIZE, 3))\n",
    "\n",
    "# # Congelar todas las capas de Xception inicialmente\n",
    "# for layer in base_model.layers:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# # Construir el modelo completo\n",
    "# modelVGG16 = Sequential([\n",
    "#     base_model,\n",
    "#     GlobalAveragePooling2D(),\n",
    "#     Dense(256),\n",
    "#     LeakyReLU(alpha=0.1),\n",
    "#     Dropout(0.25),\n",
    "#     Dense(128),\n",
    "#     LeakyReLU(alpha=0.1),\n",
    "#     Dropout(0.25),\n",
    "#     Dense(100),\n",
    "#     LeakyReLU(alpha=0.1),\n",
    "#     Dense(len(CATEGORIAS), activation='softmax')\n",
    "# ])\n",
    "\n",
    "# # Compilar el modelo con Adam\n",
    "# modelVGG16.compile(\n",
    "#     optimizer=Adam(),\n",
    "#     loss='categorical_crossentropy',\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "\n",
    "# # Entrenar solo las capas superiores\n",
    "# historyVGG = modelVGG16.fit(\n",
    "#     train,\n",
    "#     epochs=5,\n",
    "#     batch_size=128,\n",
    "#     validation_data=validation,\n",
    "#     callbacks=[OurCustomCallback()]\n",
    "# )\n",
    "\n",
    "# # Descongelar y hacer fine-tuning en las últimas capas de Xception\n",
    "# # Imprimir los nombres de capas y sus índices para decidir el ajuste fino\n",
    "# for i, layer in enumerate(base_model.layers):\n",
    "#     print(i, layer.name)\n",
    "\n",
    "# # Descongelar las últimas N capas (por ejemplo, 36 capas superiores)\n",
    "# for layer in base_model.layers[-36:]:\n",
    "#     layer.trainable = True\n",
    "\n",
    "# # Compilar de nuevo con un optimizador de menor tasa de aprendizaje\n",
    "# modelVGG16.compile(\n",
    "#     optimizer=SGD(learning_rate=0.0001, momentum=0.9),\n",
    "#     loss='categorical_crossentropy',\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "\n",
    "# # Reentrenar el modelo con ajuste fino en las capas superiores de Xception\n",
    "# historyVGG_fine = modelVGG16.fit(\n",
    "#     train,\n",
    "#     epochs=10,  # Puedes ajustar este número según lo necesario\n",
    "#     batch_size=128,\n",
    "#     validation_data=validation,\n",
    "#     callbacks=[OurCustomCallback()]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelVGG16.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "modelVGG16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights_at_epochs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyVGG = modelVGG16.fit(\n",
    "    train,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    validation_data=validation,\n",
    "    callbacks=[OurCustomCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(historyVGG.history['accuracy'], label='train')\n",
    "plt.plot(historyVGG.history['val_accuracy'], label='validation')\n",
    "plt.title('Accuracy over train epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.xlim(-1, 12)\n",
    "# plt.ylim(0.5, 1.0)\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado lo visto en los resultados del modelo VGG16 obtuvimos que:\n",
    "\n",
    "- Epoch 1/5\n",
    "  356s - accuracy: 0.6932 - loss: 0.7998 - val_accuracy: 0.8081 - val_loss: 0.5265\n",
    "- Epoch 2/5\n",
    "  387s - accuracy: 0.8632 - loss: 0.3760 - val_accuracy: 0.8487 - val_loss: 0.4195\n",
    "- Epoch 3/5\n",
    "  380s - accuracy: 0.8873 - loss: 0.3106 - val_accuracy: 0.7666 - val_loss: 0.6069\n",
    "- Epoch 4/5\n",
    "  360s - accuracy: 0.8931 - loss: 0.2758 - val_accuracy: 0.8400 - val_loss: 0.4288\n",
    "- Epoch 5/5\n",
    "  359s - accuracy: 0.8969 - loss: 0.2680 - val_accuracy: 0.8342 - val_loss: 0.4899\n",
    "\n",
    "Elegimos la epoca 2 dado que en las siguientes el modelo overfitea porque el accuracy de validation empieza a caer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prueba LeakyRelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "pretrained_model = VGG16(input_shape=input_shape, include_top=False)\n",
    "pretrained_model.trainable = False\n",
    "\n",
    "model_leaky_relu = Sequential([\n",
    "    pretrained_model,\n",
    "    Flatten(),\n",
    "    Dense(128),\n",
    "    LeakyReLU (alpha=0.1),\n",
    "    Dense(len(CATEGORIAS), activation='softmax'),\n",
    "])\n",
    "\n",
    "model_leaky_relu.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "model_leaky_relu.summary()\n",
    "\n",
    "model_weights_at_epochs = {}\n",
    "\n",
    "historyLeakyRelu = model_leaky_relu.fit(\n",
    "    train,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    validation_data=validation,\n",
    "    callbacks=[OurCustomCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(historyLeakyRelu.history['accuracy'], label='train')\n",
    "plt.plot(historyLeakyRelu.history['val_accuracy'], label='validation')\n",
    "plt.title('Accuracy over train epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.xlim(-1, 4)\n",
    "# plt.ylim(0.5, 1.0)\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ahora el LeakyRelu es el mejor resultado obtenido, donde en la epoca 5 se puede obtener un accuracy del 0.91. \n",
    "\n",
    "Tambien se probó cambiando los parametros del images_reader para ver si se obtenian mejores resultados pero no fue el caso, sino que se obtuvo un 0.82 en la epoca 5.\n",
    "\n",
    "images_reader = ImageDataGenerator(\n",
    "    \n",
    "    rescale=1/255,\n",
    "    rotation_range=10,\n",
    "    brightness_range=(0.5, 1.5),\n",
    "    width_shift_range=0.3, *\n",
    "    height_shift_range=0.3, *\n",
    "    horizontal_flip=True, *\n",
    "    fill_mode='nearest' *\n",
    ") \n",
    "\n",
    "*Son los parametros que se agregaron en comparacion a la primer prueba del LeakyRelu. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelConvolucional.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "modelConvolucional.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights_at_epochs = {}\n",
    "\n",
    "class OurCustomCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        model_weights_at_epochs[epoch] = self.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyCon = modelConvolucional.fit(\n",
    "    train,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    validation_data=validation,\n",
    "    callbacks=[OurCustomCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos el accuracy de ambos conjuntos, tanto train como validation, durante todo el proceso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(historyCon.history['accuracy'], label='train')\n",
    "plt.plot(historyCon.history['val_accuracy'], label='validation')\n",
    "plt.title('Accuracy over train epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elegimos la que consideremos como la mejor epoca y nos quedamos con ese conjunto de pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_EPOCH = 4\n",
    "# modelConvolucional.set_weights(model_weights_at_epochs[BEST_EPOCH])\n",
    "# modelMLP.set_weights(model_weights_at_epochs[BEST_EPOCH])\n",
    "# model_leaky_relu.set_weights(model_weights_at_epochs[BEST_EPOCH])\n",
    "modelVGG16.set_weights(model_weights_at_epochs[BEST_EPOCH])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora analizamos el error de ambos conjuntos para sacar nuestras propias conclusiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = (\n",
    "    ('train', images_reader.flow_from_directory(TRAIN_DIR, **READ_PARAMS, batch_size=-1)),\n",
    "    ('validation', images_reader.flow_from_directory(VALIDATION_DIR, **READ_PARAMS, batch_size=-1)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for dataset_name, dataset in datasets:\n",
    "    print('#' * 25, dataset_name, '#' * 25)\n",
    "\n",
    "    batch_images, batch_labels = next(dataset)\n",
    "    \n",
    "    # super importante: usamos argmax para convertir cosas de este formato:\n",
    "    # [(0, 1, 0), (1, 0, 0), (1, 0, 0), (0, 0, 1)]\n",
    "    # a este formato (donde tenemos el índice de la clase que tiene número más alto):\n",
    "    # [1, 0, 0, 2]\n",
    "    predictions = np.argmax(modelMLP.predict(batch_images), axis=-1)\n",
    "    labels = np.argmax(batch_labels, axis=-1)\n",
    "    \n",
    "    print('Accuracy:', accuracy_score(labels, predictions))\n",
    "\n",
    "    # graficamos la confussion matrix\n",
    "    plt.figure(figsize=(3, 4))\n",
    "        \n",
    "    plt.xticks([0, 1, 2, 3, 4, 5], CATEGORIAS, rotation=45)\n",
    "    plt.yticks([0, 1, 2, 3, 4, 5], CATEGORIAS)\n",
    "    plt.xlabel('Predicted class')\n",
    "    plt.ylabel('True class')\n",
    "\n",
    "    plt.imshow(\n",
    "        confusion_matrix(labels, predictions), \n",
    "        cmap=plt.cm.Blues,\n",
    "        interpolation='nearest',\n",
    "    )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name, dataset in datasets:\n",
    "    print('#' * 25, dataset_name, '#' * 25)\n",
    "\n",
    "    batch_images, batch_labels = next(dataset)\n",
    "    \n",
    "    # super importante: usamos argmax para convertir cosas de este formato:\n",
    "    # [(0, 1, 0), (1, 0, 0), (1, 0, 0), (0, 0, 1)]\n",
    "    # a este formato (donde tenemos el índice de la clase que tiene número más alto):\n",
    "    # [1, 0, 0, 2]\n",
    "    predictions = np.argmax(modelConvolucional.predict(batch_images), axis=-1)\n",
    "    labels = np.argmax(batch_labels, axis=-1)\n",
    "    \n",
    "    print('Accuracy:', accuracy_score(labels, predictions))\n",
    "\n",
    "    # graficamos la confussion matrix\n",
    "    plt.figure(figsize=(3, 4))\n",
    "        \n",
    "    plt.xticks([0, 1, 2, 3, 4, 5], CATEGORIAS, rotation=45)\n",
    "    plt.yticks([0, 1, 2, 3, 4, 5], CATEGORIAS)\n",
    "    plt.xlabel('Predicted class')\n",
    "    plt.ylabel('True class')\n",
    "\n",
    "    plt.imshow(\n",
    "        confusion_matrix(labels, predictions), \n",
    "        cmap=plt.cm.Blues,\n",
    "        interpolation='nearest',\n",
    "    )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name, dataset in datasets:\n",
    "    print('#' * 25, dataset_name, '#' * 25)\n",
    "\n",
    "    batch_images, batch_labels = next(dataset)\n",
    "    \n",
    "    # super importante: usamos argmax para convertir cosas de este formato:\n",
    "    # [(0, 1, 0), (1, 0, 0), (1, 0, 0), (0, 0, 1)]\n",
    "    # a este formato (donde tenemos el índice de la clase que tiene número más alto):\n",
    "    # [1, 0, 0, 2]\n",
    "    predictions = np.argmax(modelVGG16.predict(batch_images), axis=-1)\n",
    "    labels = np.argmax(batch_labels, axis=-1)\n",
    "    \n",
    "    print('Accuracy:', accuracy_score(labels, predictions))\n",
    "\n",
    "    # graficamos la confussion matrix\n",
    "    plt.figure(figsize=(3, 4))\n",
    "        \n",
    "    plt.xticks([0, 1, 2, 3, 4, 5], CATEGORIAS, rotation=45)\n",
    "    plt.yticks([0, 1, 2, 3, 4, 5], CATEGORIAS)\n",
    "    plt.xlabel('Predicted class')\n",
    "    plt.ylabel('True class')\n",
    "\n",
    "    plt.imshow(\n",
    "        confusion_matrix(labels, predictions), \n",
    "        cmap=plt.cm.Blues,\n",
    "        interpolation='nearest',\n",
    "    )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name, dataset in datasets:\n",
    "    print('#' * 25, dataset_name, '#' * 25)\n",
    "\n",
    "    batch_images, batch_labels = next(dataset)\n",
    "    \n",
    "    # super importante: usamos argmax para convertir cosas de este formato:\n",
    "    # [(0, 1, 0), (1, 0, 0), (1, 0, 0), (0, 0, 1)]\n",
    "    # a este formato (donde tenemos el índice de la clase que tiene número más alto):\n",
    "    # [1, 0, 0, 2]\n",
    "    predictions = np.argmax(model_leaky_relu.predict(batch_images), axis=-1)\n",
    "    labels = np.argmax(batch_labels, axis=-1)\n",
    "    \n",
    "    print('Accuracy:', accuracy_score(labels, predictions))\n",
    "\n",
    "    # graficamos la confussion matrix\n",
    "    plt.figure(figsize=(3, 4))\n",
    "        \n",
    "    plt.xticks([0, 1, 2, 3, 4, 5], CATEGORIAS, rotation=45)\n",
    "    plt.yticks([0, 1, 2, 3, 4, 5], CATEGORIAS)\n",
    "    plt.xlabel('Predicted class')\n",
    "    plt.ylabel('True class')\n",
    "\n",
    "    plt.imshow(\n",
    "        confusion_matrix(labels, predictions), \n",
    "        cmap=plt.cm.Blues,\n",
    "        interpolation='nearest',\n",
    "    )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa en los gráficos de confusión que el modelo presenta problemas a la hora de tener que clasificar montañas, prediciendo que son oceanos o glaciares. Por ejemplo, al probar el modelo con la imagen 20058.jpg se muestra la prediccion erronea que planteamos.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ahora probaremos con nuestras propias imágenes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "def show_and_predict(image_path):\n",
    "    image_array = img_to_array(load_img(image_path, target_size=(SIZE, SIZE)))\n",
    "    inputs = np.array([image_array])  # armamos un \"dataset\" con solo esa imagen\n",
    "    predictions = model_leaky_relu.predict(inputs)\n",
    "    display(Image(image_path, width=500))\n",
    "    print(\"Prediction:\", CATEGORIAS[np.argmax(predictions)])\n",
    "    print(\"Prediction detail:\", predictions)\n",
    "show_and_predict(\"./test/20070.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "CATEGORIA2 = ['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n",
    "SIZE = 150\n",
    "\n",
    "def predict_images_from_directory(directory):\n",
    "    results = [] \n",
    "    images = [] \n",
    "\n",
    "    for image_name in os.listdir(directory):\n",
    "        image_path = os.path.join(directory, image_name)\n",
    "\n",
    "        if image_name.endswith(\".jpg\"):\n",
    "            image_array = img_to_array(load_img(image_path, target_size=(SIZE, SIZE)))\n",
    "            images.append(image_array)\n",
    "\n",
    "    inputs = np.array(images) / 255.0  \n",
    "\n",
    "    predictions = modelVGG16.predict(inputs)\n",
    "\n",
    "    for i, image_name in enumerate(os.listdir(directory)):\n",
    "        if image_name.endswith(\".jpg\"):\n",
    "            predicted_class = CATEGORIA2[np.argmax(predictions[i])]\n",
    "            results.append([image_name, predicted_class])\n",
    "    \n",
    "    df = pd.DataFrame(results, columns=[\"ID\", \"Label\"])\n",
    "    df.to_csv(\"prediccionesXceptionModificaDos.csv\", index=False)\n",
    "    print(\"Predicciones guardadas en 'prediccionesXceptionModificaDos.csv'\")\n",
    "\n",
    "predict_images_from_directory('test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusiones"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
