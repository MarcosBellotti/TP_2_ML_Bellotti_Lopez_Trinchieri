{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo Practico Nº2 - Machine learning - Los convolucionales - Bellotti, Lopez, Trinchieri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports iniciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, Convolution2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuraciones iniciales de algunas constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIAS = 'buildings', 'forest', 'glacier', 'mountain', 'sea', 'street'\n",
    "\n",
    "TRAIN_DIR = Path('train')\n",
    "VALIDATION_DIR='validation'\n",
    "TEST_DIR = Path('test')\n",
    "SIZE = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de la calidad de las imágenes que serán utilizadas para entrenar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de comenzar a trabajar, revisamos en general las imagenes de train y nos encontramos con muchas imagenes que nada tenían que ver con la categoría a la que decian pertenecer, principalmente en **glacier**. Las mismas fueron removidas de dicho directorio y las guardamos en la carpeta deletes.\n",
    "\n",
    "Un ejemplo para que se comprenda el tipo de imágenes que quitamos, es el de una persona disfrazada de dinosaurio en el gran cañón categorizada como **glacier**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividimos las imagenes de train en dos dataset de train y validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desarrollamos una función que extrae el primer 20% de las imagenes de cada categoría para que pasen a ser imagenes de validation. Utilizamos el porcentaje y no una cantidad fija para mantener las proporciones, además quitamos el primer 20% para que todos tengamos el mismo set de train y valdiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def dividir_train_validation_existente(carpeta_train, carpeta_validation, porcentaje_validacion=0.2):\n",
    "    if not os.path.exists(carpeta_validation):\n",
    "        os.makedirs(carpeta_validation)\n",
    "\n",
    "    for categoria in os.listdir(carpeta_train):\n",
    "        ruta_categoria_train = os.path.join(carpeta_train, categoria)\n",
    "   \n",
    "        if os.path.isdir(ruta_categoria_train):\n",
    "            imagenes = [img for img in os.listdir(ruta_categoria_train) if os.path.isfile(os.path.join(ruta_categoria_train, img))]\n",
    "\n",
    "            num_validacion = int(len(imagenes) * porcentaje_validacion)\n",
    "    \n",
    "            ruta_categoria_validation = os.path.join(carpeta_validation, categoria)\n",
    "            if not os.path.exists(ruta_categoria_validation):\n",
    "                os.makedirs(ruta_categoria_validation)\n",
    "\n",
    "            for i in range(num_validacion):\n",
    "                imagen = imagenes[i]\n",
    "                ruta_imagen = os.path.join(ruta_categoria_train, imagen)\n",
    "                shutil.move(ruta_imagen, os.path.join(ruta_categoria_validation, imagen))\n",
    "\n",
    "    print(\"División completada: 20% de las imágenes movidas a validation.\")\n",
    "\n",
    "carpeta_train = 'train'\n",
    "carpeta_validation = 'validation'\n",
    "\n",
    "dividir_train_validation_existente(carpeta_train, carpeta_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis exploratorio del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def contar_imagenes_por_categoria(carpeta_train):\n",
    "    conteo_categorias = {}\n",
    "\n",
    "    for categoria in os.listdir(carpeta_train):\n",
    "        ruta_categoria = os.path.join(carpeta_train, categoria)\n",
    "\n",
    "        if os.path.isdir(ruta_categoria):\n",
    "            cantidad_imagenes = len([img for img in os.listdir(ruta_categoria) if os.path.isfile(os.path.join(ruta_categoria, img))])\n",
    "            conteo_categorias[categoria] = cantidad_imagenes\n",
    "\n",
    "    return conteo_categorias\n",
    "\n",
    "def graficar_distribucion(conteo_categorias):\n",
    "    categorias = list(conteo_categorias.keys())\n",
    "    cantidades = list(conteo_categorias.values())\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=categorias, y=cantidades, palette='viridis')\n",
    "\n",
    "\n",
    "    plt.title('Distribución de Imágenes por Categoría en la Carpeta Test', fontsize=14)\n",
    "    plt.xlabel('Categoría', fontsize=12)\n",
    "    plt.ylabel('Cantidad de Imágenes', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "conteo_categorias = contar_imagenes_por_categoria(TRAIN_DIR)\n",
    "\n",
    "graficar_distribucion(conteo_categorias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. *Volumetría de los datos y distribución de las variables a predecir:*\n",
    "   El conjunto de datos cuenta con un total aproximado de 14,000 imágenes, distribuidas en 6 categorías, de la siguiente manera:\n",
    "\n",
    "   - *Buildings*: 15.6%\n",
    "   - *Forest*: 16.2%\n",
    "   - *Glacier*: 17.1%\n",
    "   - *Mountain*: 17.9%\n",
    "   - *Sea*: 16.2%\n",
    "   - *Street*: 17.0%\n",
    "\n",
    "   Las categorías están relativamente balanceadas, con diferencias menores en el número de imágenes entre clases. Ninguna categoría domina el conjunto de datos, lo que favorece el entrenamiento de un modelo equilibrado. Esto implica que no sería necesario realizar ajustes significativos para balancear las clases en este punto.\n",
    "\n",
    "2. *Estructura y tipo de las imágenes:*\n",
    "   - Las imágenes tienen un tamaño de *150x150 píxeles*.\n",
    "   - El formato de las imágenes es JPG.\n",
    "   - Las imágenes pertenecen a paisajes y escenas específicas de categorías bien diferenciadas, como edificios, naturaleza, y calles.\n",
    "\n",
    "\n",
    "**Aclaración:** la distrución, luego de dividir train en dos sets nuevos, sigue siendo la misma proporción dado que nos quedamos con el 20% de cada categoría en validation y el 80% de cada categoría en train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Al incio utilizamos un generador de imágenes reescaladas, con el brillo cambiado y rotadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_reader_legacy = ImageDataGenerator(\n",
    "     rescale=1/255,\n",
    "     rotation_range=10,\n",
    "     brightness_range=(0.5, 1.5)  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dicho generador de imágenes no dió resultados óptimos, probamos con el siguiente, que tiene más características:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mismo aplica una serie de transformaciones a las imágenes de entrada para realizar aumento de datos, mejorando la generalización del modelo. Normaliza los valores de los píxeles, rota, desplaza y aplica zoom a las imágenes de forma aleatoria, además de ajustar el brillo y permitir volteo horizontal. También realiza distorsiones de corte (shear) y rellena los píxeles vacíos generados por estas transformaciones con el valor más cercano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_reader = ImageDataGenerator(\n",
    "    rescale=1/255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    brightness_range=(0.5, 1.5),\n",
    "    horizontal_flip=True,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "  \n",
    "READ_PARAMS = dict(\n",
    "    class_mode=\"categorical\",\n",
    "    classes=CATEGORIAS,\n",
    "    target_size=(SIZE, SIZE),\n",
    "    color_mode=\"rgb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inicializamos los generadores de train y validation para utilizar durante el entrenamiento de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = images_reader.flow_from_directory(TRAIN_DIR, **READ_PARAMS)\n",
    "validation = images_reader.flow_from_directory(VALIDATION_DIR, **READ_PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplos de imagenes generadas a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(dataset):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    images, labels = next(dataset)\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i])\n",
    "        plt.title(CATEGORIAS[np.argmax(labels[i])])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplos de train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_images(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplos de validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_images(validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de los modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos inicialmente la forma de las imagenes de entrada (input_shape) y el código encargado de almacenar los pesos del modelo al final de cada época, dado que son constantes que utilizaremos en los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (SIZE, SIZE, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights_at_epochs = {}\n",
    "\n",
    "class OurCustomCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        model_weights_at_epochs[epoch] = self.model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **MLP** - Multi layer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aclaración:\n",
    "\n",
    "Este modelo lo vamos a entrenar solo para probar, de antemano sabemos que será un modelo que no dará buenos resultados con el tipo de problema que estamos trabajando... para ello necesitamos CNN (Redes convolucionales)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la **arquitectura** del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelMLP = Sequential([\n",
    "    Input(input_shape),\n",
    "    \n",
    "    Flatten(),\n",
    "\n",
    "    Dense(500, activation='tanh'),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Dense(len(CATEGORIAS), activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo compilamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelMLP.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "modelMLP.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el MLP con 5 epocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyMLP = modelMLP.fit(\n",
    "    train,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    validation_data=validation,\n",
    "    callbacks=[OurCustomCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusión:\n",
    "\n",
    "Como comentamos anteriormente, los MLPs suelen ser menos eficientes que las redes convolucionales (CNN) para este tipo de tareas, ya que no están diseñados específicamente para capturar las jerarquías espaciales y las características locales en las imágenes. Y esta teoría se refuerza con la práctica, dado que si observamos la primer entrega que realizamos en el kaggle, el accuracy del modelo fue de 0.30 (referencia de la entrega de kaggle --> predicciones.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Convolucional**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo convolucional básico, utilizando funciones de activación relu y una capa densa con funcion de activación tanh (la cual no es indicada para clasificar 6 categorías, es preferible softmax o relu, como se utiliza en la siguiente capa densa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights_at_epochs = {}\n",
    "\n",
    "modelConvolucional = Sequential([\n",
    "    Input(input_shape),\n",
    "\n",
    "    Convolution2D(filters=10, kernel_size=(4, 4), strides=1, activation='relu'),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Convolution2D(filters=10, kernel_size=(4, 4), strides=1, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    MaxPooling2D(pool_size=(4, 4)),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(100, activation='tanh'),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Dense(len(CATEGORIAS), activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelConvolucional.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "modelConvolucional.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights_at_epochs = {}\n",
    "\n",
    "historyConvolutional = modelConvolucional.fit(\n",
    "    train,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    validation_data=validation,\n",
    "    callbacks=[OurCustomCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(No generamos un .csv dado que los resultados no fueron buenos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolucional modificado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este modelo convolucional utiliza más filtros (32, 64 y 128) para capturar características detalladas en diferentes niveles de abstracción, mientras que los kernels más pequeños de 3x3 permiten una mejor eficiencia en la detección de patrones locales. Las capas de MaxPooling con un tamaño de 2x2 reducen el tamaño de las características progresivamente, ayudando a prevenir el sobreajuste. La activación relu en las capas densas mejora la eficiencia en la propagación de gradientes, y el Dropout más alto (0.5) en las últimas capas evita el sobreajuste, lo que hace al modelo más robusto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelConvolucionalV2 = Sequential([\n",
    "    Input(shape=(150, 150, 3)),\n",
    "\n",
    "    Convolution2D(filters=32, kernel_size=(3, 3), strides=1, activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Convolution2D(filters=64, kernel_size=(3, 3), strides=1, activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Convolution2D(filters=128, kernel_size=(3, 3), strides=1, activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    Dense(len(CATEGORIAS), activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelConvolucionalV2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "modelConvolucionalV2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights_at_epochs = {}\n",
    "\n",
    "historyConvolutionalV2 = modelConvolucionalV2.fit(\n",
    "    train,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    validation_data=validation,\n",
    "    callbacks=[OurCustomCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusión acerca de los modelos convolucionales NO preentrenados\n",
    "\n",
    "Hemos realizado diversas pruebas cambiando la cantidad de epocas de entrenamiento, modificando la arquitectura, los generadores de imagenes y demás, y los mejores resultados fueron de 0.77 (prediccionesVGG.csv) de accuracy en nuestras pruebas.\n",
    "Consideramos que el uso de modelos preentrenados como VGG16 puede conducir a resultados significativamente mejores.\n",
    "\n",
    "Por ello, damos paso a trabajar con los siguientes modelos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **VGG16**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, probamos con un VGG16 básico con una arquitectura con función de activación tanh (que antes mencionamos que no servía), con el fin de validar si de todas maneras obteníamos mejores resultados que una red sin preentrenar y, para nuestra sorpresa, así fue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = VGG16(input_shape=input_shape, include_top=False)\n",
    "pretrained_model.trainable = False\n",
    "\n",
    "modelVGG16 = Sequential([\n",
    "    pretrained_model,\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(100, activation='tanh'),\n",
    "    Dense(100, activation='tanh'),\n",
    "    \n",
    "    Dense(len(CATEGORIAS), activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelVGG16.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "modelVGG16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights_at_epochs = {}\n",
    "\n",
    "historyModelVGG16 = modelVGG16.fit(\n",
    "    train,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    validation_data=validation,\n",
    "    callbacks=[OurCustomCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con VGG16 obtuvimos un accuracy del 0.80 (predicciones.csv) pero, aun así, queríamos mejorar y sabíamos que podríamos dado que la arquitectura definida era básica, por eso nos pusimos a investigar que arquitectura sería mejor y, a su vez, continuamos buscando modelos más potentes que nos permitan seguir mejorando.\n",
    "\n",
    "\n",
    "El primer intento fue cambiar la función de activación por relu, que era mas adecuada al tipo de problema que estabamos trabajando. De esta manera obtuvimos un 0.86 (prediccionesVGGRelu.csv) de accuracy, lo que nos indicó que ibamos por buen camino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(input_shape=input_shape, include_top=False)\n",
    "pretrained_model.trainable = False\n",
    "\n",
    "modelVGG16_relu = Sequential([\n",
    "    base_model,\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5), \n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(len(CATEGORIAS), activation='softmax')\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelVGG16_relu.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "modelVGG16_relu.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights_at_epochs = {}\n",
    "\n",
    "historyModelVGG16_relu = modelVGG16_relu.fit(\n",
    "    train,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    validation_data=validation,\n",
    "    callbacks=[OurCustomCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontramos que leakyRelu como función de activación era mejor que relu sola dado que soluciona el problema \"muerte de neuronas\". \n",
    "\n",
    "Lo que ocurre es que las neuronas pueden quedar inactivas y no aprender si sus entradas son negativas, ya que la función devuelve cero para esas entradas. Leaky ReLU permite un pequeño valor negativo (0.01) para las entradas negativas, lo que permite que las neuronas continúen activas y aprendan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "pretrained_model = VGG16(input_shape=input_shape, include_top=False)\n",
    "pretrained_model.trainable = False\n",
    "\n",
    "modelVGG16_leaky_relu = Sequential([\n",
    "    pretrained_model,\n",
    "    Flatten(),\n",
    "    Dense(128),\n",
    "    LeakyReLU (alpha=0.1),\n",
    "    Dense(len(CATEGORIAS), activation='softmax'),\n",
    "])\n",
    "\n",
    "modelVGG16_leaky_relu.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "modelVGG16_leaky_relu.summary()\n",
    "\n",
    "model_weights_at_epochs = {}\n",
    "\n",
    "historyLeakyRelu = modelVGG16_leaky_relu.fit(\n",
    "    train,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    validation_data=validation,\n",
    "    callbacks=[OurCustomCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusiones VGG16:\n",
    "\n",
    "A partir de la utilización de un modelo preentrenado ya pudimos observar una gran mejora en las respuestas del modelo, y modificando la función de activación principalmente es dónde encontramos la ventaja para el mismo, dado que cuando modifcamos la arquitectura (cantidad de capas, tipos, dropout, etc) no se notaba demasiado la mejora en accuracy.\n",
    "\n",
    "El mejor resultado obtenido con el mismo es 0.87934 (prediccionesLeakyRelu2.csv).\n",
    "\n",
    "De todas maneras, estabamos estancados porque no podíamos mejorar el accuracy. \n",
    "\n",
    "Por esto, investigamos y encontramos que existen más modelos preentrenados, y que son más potentes y eficientes para entrenar. Nosotros elegimos el siguiente:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Xception**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En principio, definimos el siguiente modelo base, al cual le fuimos modificando la arquitectura (cantidad y tipos de capas, droput o no, funciones de activación, etc) y modificando también los generadores de imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import Xception\n",
    "\n",
    "base_model = Xception(weights='imagenet', include_top=False, input_shape=(SIZE,SIZE,3))\n",
    "base_model.trainable = False\n",
    "\n",
    "modelXception = Sequential([\n",
    "    base_model,\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    \n",
    "    Dense(len(CATEGORIAS), activation='softmax'),\n",
    "])\n",
    "\n",
    "modelXception.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "modelXception.summary()\n",
    "\n",
    "model_weights_at_epochs = {}\n",
    "\n",
    "historyXception= modelXception.fit(\n",
    "    train,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    validation_data=validation,\n",
    "    callbacks=[OurCustomCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Según lo comentado anteriormente, obtuvimos los siguientes resultados (por orden de entrega en Kaggle):\n",
    "\n",
    "accuracy 0.889 - (prediccionesXception.csv)\n",
    "accuracy 0.881 - (prediccionesXception6.csv)\n",
    "accuracy 0.888 - (prediccionesXception11.csv)\n",
    "accuracy 0.882 - (prediccionesXception19.csv) <-- Había overfiteado\n",
    "accuracy 0.89 - (prediccionesXceptionEpoch8.csv)\n",
    "\n",
    "El siguiente tenía le agregamos solo una capa densa más y mejoró: \n",
    "accuracy 0.906 - (prediccionesXceptionEpoch9.csv)\n",
    "\n",
    "Luego, por ejemplo, limpiamos el dataset con lo comentado al comienzo y mejoró suitlmente:\n",
    "accuracy 0.9088 - (prediccionesXceptionDataSetClean.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pero, finalmente, conseguimos la siguiente arquitectura:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "base_model = Xception(weights='imagenet', include_top=False, input_shape=(SIZE,SIZE,3))\n",
    "base_model.trainable = False\n",
    "\n",
    "modelXceptionModified = Sequential([\n",
    "    base_model,\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(256),\n",
    "    LeakyReLU(alpha=0.1),  \n",
    "    Dropout(0.25),\n",
    "    Dense(128),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    Dropout(0.25),\n",
    "    Dense(100),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    Dense(len(CATEGORIAS), activation='softmax')\n",
    "])\n",
    "\n",
    "modelXceptionModified.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "modelXceptionModified.summary()\n",
    "\n",
    "model_weights_at_epochs = {}\n",
    "\n",
    "historyXceptionModified = modelXceptionModified.fit(\n",
    "    train,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_data=validation,\n",
    "    callbacks=[OurCustomCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La arquitectura tiene 4 capas densas que permiten aprender relaciones profundas, además utilizamos la función de activación LeakyReLU que, como comentamos antes, es mejor que relu porque asegura que las neuronas permanezcan activas durante el entrenamiento y no se \"apaguen\". \n",
    "Además, agregamos capas de Dropout para evitar el overfiteo, promoviendo que el modelo aprenda representaciones más robustas y generales. \n",
    "Y, finalmente, softmax como función de activación para la capa de salida.\n",
    "\n",
    "\n",
    "Con esta arquitectura, el **accuracy del modelo es de 0.933 (prediccionesXceptionModificado.csv)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función para análizar el entrenamiento y seleccionar la mejor época de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = historyXceptionModified\n",
    "\n",
    "\n",
    "plt.plot(model.history['accuracy'], label='train')\n",
    "plt.plot(model.history['val_accuracy'], label='validation')\n",
    "plt.title('Accuracy over train epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.xlim(-1, 12)\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elegimos la que consideremos como la mejor epoca y nos quedamos con esos de pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_EPOCH = 4\n",
    "# modelMLP.set_weights(model_weights_at_epochs[BEST_EPOCH])\n",
    "# modelConvolucional.set_weights(model_weights_at_epochs[BEST_EPOCH])\n",
    "# modelConvolucionalV2.set_weights(model_weights_at_epochs[BEST_EPOCH])\n",
    "# modelVGG16.set_weights(model_weights_at_epochs[BEST_EPOCH])\n",
    "# modelVGG16_relu.set_weights(model_weights_at_epochs[BEST_EPOCH])\n",
    "# modelVGG16_leaky_relu.set_weights(model_weights_at_epochs[BEST_EPOCH])\n",
    "# modelXception.set_weights(model_weights_at_epochs[BEST_EPOCH])\n",
    "\n",
    "modelXceptionModified.set_weights(model_weights_at_epochs[BEST_EPOCH])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora analizamos el error de ambos conjuntos para sacar nuestras propias conclusiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = (\n",
    "    ('train', images_reader.flow_from_directory(TRAIN_DIR, **READ_PARAMS, batch_size=-1)),\n",
    "    ('validation', images_reader.flow_from_directory(VALIDATION_DIR, **READ_PARAMS, batch_size=-1)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matriz de confusión**\n",
    "\n",
    "\n",
    "Gráficamos esta matriz al finalizar cada entrenamiento de modelo con el fin de comprender en mayor profundidad sus respuestas.\n",
    "\n",
    "(debemos modificar el model con el modelo entrenado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = modelXceptionModified\n",
    "\n",
    "for dataset_name, dataset in datasets:\n",
    "    print('#' * 25, dataset_name, '#' * 25)\n",
    "\n",
    "    batch_images, batch_labels = next(dataset)\n",
    "    \n",
    "    predictions = np.argmax(model.predict(batch_images), axis=-1)\n",
    "    labels = np.argmax(batch_labels, axis=-1)\n",
    "    \n",
    "    print('Accuracy:', accuracy_score(labels, predictions))\n",
    "\n",
    "    plt.figure(figsize=(3, 4))\n",
    "        \n",
    "    plt.xticks([0, 1, 2, 3, 4, 5], CATEGORIAS, rotation=45)\n",
    "    plt.yticks([0, 1, 2, 3, 4, 5], CATEGORIAS)\n",
    "    plt.xlabel('Predicted class')\n",
    "    plt.ylabel('True class')\n",
    "\n",
    "    plt.imshow(\n",
    "        confusion_matrix(labels, predictions), \n",
    "        cmap=plt.cm.Blues,\n",
    "        interpolation='nearest',\n",
    "    )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pruebas con imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la siguiente función probamos, a partir del modelo entrenado, que respuesta nos da a una imagen en particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "model = modelXceptionModified\n",
    "\n",
    "def show_and_predict(image_path):\n",
    "    image_array = img_to_array(load_img(image_path, target_size=(SIZE, SIZE)))\n",
    "    inputs = np.array([image_array])\n",
    "    predictions = model.predict(inputs)\n",
    "    display(Image(image_path, width=500))\n",
    "    print(\"Prediction:\", CATEGORIAS[np.argmax(predictions)])\n",
    "    print(\"Prediction detail:\", predictions)\n",
    "show_and_predict(\"./test/20070.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función para pasar a .csv las predicciones del modelo con el set de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "CATEGORIA2 = ['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n",
    "SIZE = 150\n",
    "\n",
    "def predict_images_from_directory(directory):\n",
    "    results = [] \n",
    "    images = [] \n",
    "\n",
    "    for image_name in os.listdir(directory):\n",
    "        image_path = os.path.join(directory, image_name)\n",
    "\n",
    "        if image_name.endswith(\".jpg\"):\n",
    "            image_array = img_to_array(load_img(image_path, target_size=(SIZE, SIZE)))\n",
    "            images.append(image_array)\n",
    "\n",
    "    inputs = np.array(images) / 255.0  \n",
    "\n",
    "    predictions = modelXceptionModified.predict(inputs)\n",
    "\n",
    "    for i, image_name in enumerate(os.listdir(directory)):\n",
    "        if image_name.endswith(\".jpg\"):\n",
    "            predicted_class = CATEGORIA2[np.argmax(predictions[i])]\n",
    "            results.append([image_name, predicted_class])\n",
    "    \n",
    "    df = pd.DataFrame(results, columns=[\"ID\", \"Label\"])\n",
    "    df.to_csv(\"prediccionesXceptionModificaDos.csv\", index=False)\n",
    "    print(\"Predicciones guardadas en 'prediccionesXceptionModificaDos.csv'\")\n",
    "\n",
    "predict_images_from_directory('test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como era de esperar, dada la teoría, los modelos **MLP** tienen limitaciones para capturar las características espaciales y jerárquicas en imágenes. Esto los hace menos eficaces para problemas complejos de clasificación de imágenes en comparación con los modelos convolucionales, por ello obtuvimos tan solo un **0.3 de accuracy** y no nos eforzamos por conseguir algo mejor dado que nuestra hipótesis es que no ibamos a llegar a buen puerto. \n",
    "\n",
    "Por otro lado, en el caso de los modelos **convolucionales básicos**, si esperábamos que sean más efectivos para extraer características espaciales de las imágenes, gracias a las operaciones de convolución y agrupamiento. Sin embargo, las arquitecturas de las redes convolucionales simples que desarrollamos no fueron lo suficientemente buenas dado que el mejor accuracy conseguido fue de **0.769**.\n",
    "\n",
    "Pero, por lo invesitgado, los modelos preentrenados como VGG16 y Xception han sido entrenados en grandes volúmenes de datos y son capaces de aprender representaciones más generales y sofisticadas, lo que les dió una ventaja con respecto a los convolucionales base que definimos nosotros. \n",
    "\n",
    "**VGG16** fue un modelo que entreno más lento y generó mejores resultados, pero no demasiados significativos, llegando a un accuracy máximo de **0.879**. \n",
    "\n",
    "**Xception**, según lo leído, es una arquitectura más avanzada basada en convoluciones separables, lo que le permite aprender representaciones más eficientes y generalizar mejor, logrando un rendimiento superior en comparación con VGG16. Dicha teoría se vió en la práctica dado que obtuvimos mejores resultados, como el accuracy del **0.93** y con un tiempo de entrenamiento menor.\n",
    "\n",
    "En resumen, mientras que los MLP no sirvieron de mucho, los modelos convolucionales básicos tuvieron un mejor rendimiento del que esperabamos previamente. Y, por su parte, VGG16 fue menos eficiente y capaz de lo que creíamos, mientras que Xception nos sorprendió en su capacidad para aprender características ricas y generales, mejorando la precisión y la eficiencia del modelo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
