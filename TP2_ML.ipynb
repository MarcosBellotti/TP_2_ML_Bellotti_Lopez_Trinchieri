{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# un poco menos de warnings de tensorflow\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# de python, para especificar rutas de archivos y directorios\n",
    "from pathlib import Path\n",
    "\n",
    "# lib para trabajar con arrays\n",
    "import numpy as np\n",
    "\n",
    "# lib que usamos para mostrar las imágenes\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# libs que usamos para construir y entrenar redes neuronales, y que además tiene utilidades para leer sets de \n",
    "# imágenes\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, Convolution2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "# libs que usamos para tareas generales de machine learning. En este caso, métricas\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# configuración para que las imágenes se vean dentro del notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuraciones iniciales de algunas constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIAS = 'buildings', 'forest', 'glacier', 'mountain', 'sea', 'street'\n",
    "\n",
    "TRAIN_DIR = Path('train')\n",
    "TEST_DIR = Path('test')\n",
    "SIZE = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Al comienzo, trabajamos con imágenes reescaladas, con el brillo cambiado y rotadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_reader = ImageDataGenerator(\n",
    "    rescale=1/255,\n",
    "    rotation_range=10,\n",
    "    brightness_range=(0.5, 1.5),\n",
    ")\n",
    "  \n",
    "READ_PARAMS = dict(\n",
    "    class_mode=\"categorical\",  # tenemos N labels, queremos tuplas de 0s y 1s indicando cuál de los labels es\n",
    "    classes=CATEGORIAS,  # para usar el mismo orden en todos lados\n",
    "    target_size=(SIZE, SIZE),  # para que corra más rápido, vamos a achicar las imágenes\n",
    "    color_mode=\"rgb\",  # queremos trabajar con las imágenes a color\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dividimos las imagenes de train en dos dataset de train y validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desarrollamos una función que extrae el 20% de las imagenes de cada categoría para que pasen a ser imagenes de validation. Utilizamos el porcentaje y no una cantidad fija para mantener las proporciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def dividir_train_validation_existente(carpeta_train, carpeta_validation, porcentaje_validacion=0.2):\n",
    "\n",
    "    if not os.path.exists(carpeta_validation):\n",
    "        os.makedirs(carpeta_validation)\n",
    "\n",
    "    for categoria in os.listdir(carpeta_train):\n",
    "        ruta_categoria_train = os.path.join(carpeta_train, categoria)\n",
    "        \n",
    "        if os.path.isdir(ruta_categoria_train):\n",
    "            imagenes = [img for img in os.listdir(ruta_categoria_train) if os.path.isfile(os.path.join(ruta_categoria_train, img))]\n",
    "\n",
    "            num_validacion = int(len(imagenes) * porcentaje_validacion)\n",
    "            \n",
    "            ruta_categoria_validation = os.path.join(carpeta_validation, categoria)\n",
    "            if not os.path.exists(ruta_categoria_validation):\n",
    "                os.makedirs(ruta_categoria_validation)\n",
    "\n",
    "            for i in range(num_validacion):\n",
    "                imagen = imagenes[i]\n",
    "                ruta_imagen = os.path.join(ruta_categoria_train, imagen)\n",
    "                shutil.move(ruta_imagen, os.path.join(ruta_categoria_validation, imagen))\n",
    "\n",
    "    print(\"División completada: 20% de las imágenes movidas a validation.\")\n",
    "\n",
    "carpeta_train = 'train'\n",
    "carpeta_validation = 'validation'\n",
    "\n",
    "dividir_train_validation_existente(carpeta_train, carpeta_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_DIR='validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = images_reader.flow_from_directory(TRAIN_DIR, **READ_PARAMS)\n",
    "validation = images_reader.flow_from_directory(VALIDATION_DIR, **READ_PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplos de imagenes a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(dataset):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    images, labels = next(dataset)\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i])\n",
    "        plt.title(CATEGORIAS[np.argmax(labels[i])])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_images(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_images(validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis exploratorio del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def contar_imagenes_por_categoria(carpeta_train):\n",
    "    conteo_categorias = {}\n",
    "\n",
    "    for categoria in os.listdir(carpeta_train):\n",
    "        ruta_categoria = os.path.join(carpeta_train, categoria)\n",
    "\n",
    "        if os.path.isdir(ruta_categoria):\n",
    "            cantidad_imagenes = len([img for img in os.listdir(ruta_categoria) if os.path.isfile(os.path.join(ruta_categoria, img))])\n",
    "            conteo_categorias[categoria] = cantidad_imagenes\n",
    "\n",
    "    return conteo_categorias\n",
    "\n",
    "def graficar_distribucion(conteo_categorias):\n",
    "    categorias = list(conteo_categorias.keys())\n",
    "    cantidades = list(conteo_categorias.values())\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=categorias, y=cantidades, palette='viridis')\n",
    "\n",
    "\n",
    "    plt.title('Distribución de Imágenes por Categoría en la Carpeta Test', fontsize=14)\n",
    "    plt.xlabel('Categoría', fontsize=12)\n",
    "    plt.ylabel('Cantidad de Imágenes', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "conteo_categorias = contar_imagenes_por_categoria(TRAIN_DIR)\n",
    "\n",
    "graficar_distribucion(conteo_categorias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. *Volumetría de los datos y distribución de las variables a predecir:*\n",
    "   El conjunto de datos cuenta con un total de 14,034 imágenes, distribuidas en 6 categorías, de la siguiente manera:\n",
    "\n",
    "   - *Buildings*: 2,191 imágenes (15.6%)\n",
    "   - *Forest*: 2,271 imágenes (16.2%)\n",
    "   - *Glacier*: 2,404 imágenes (17.1%)\n",
    "   - *Mountain*: 2,512 imágenes (17.9%)\n",
    "   - *Sea*: 2,274 imágenes (16.2%)\n",
    "   - *Street*: 2,382 imágenes (17.0%)\n",
    "\n",
    "   Las categorías están relativamente balanceadas, con diferencias menores en el número de imágenes entre clases. Ninguna categoría domina el conjunto de datos, lo que favorece el entrenamiento de un modelo equilibrado. Esto implica que no sería necesario realizar ajustes significativos para balancear las clases en este punto.\n",
    "\n",
    "2. *Estructura y tipo de las imágenes:*\n",
    "   - Las imágenes tienen un tamaño de *150x150 píxeles*.\n",
    "   - El formato de las imágenes es JPG.\n",
    "   - Las imágenes pertenecen a paisajes y escenas específicas de categorías bien diferenciadas, como edificios, naturaleza, y calles.\n",
    "\n",
    "\n",
    "**Aclaración:** la distrución, luego de dividir train en dos sets nuevos, sigue siendo la misma proporción dado que nos quedamos con el 20% de cada categoría en validation y el 80% de cada categoría en train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (SIZE, SIZE, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights_at_epochs = {}\n",
    "\n",
    "class OurCustomCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        model_weights_at_epochs[epoch] = self.model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP - Multi layer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la **arquitectura** del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelMLP = Sequential([\n",
    "    Input(input_shape),\n",
    "    \n",
    "    Flatten(),\n",
    "\n",
    "    Dense(500, activation='tanh'),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Dense(len(CATEGORIAS), activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelMLP.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "modelMLP.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el MLP con 5 epocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyMLP = modelMLP.fit(\n",
    "    train,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    validation_data=validation,\n",
    "    callbacks=[OurCustomCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graficamos la salida de la corrida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(historyMLP.history['accuracy'], label='train')\n",
    "plt.plot(historyMLP.history['val_accuracy'], label='validation')\n",
    "plt.title('Accuracy over train epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolucional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo convolucional con "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights_at_epochs = {}\n",
    "\n",
    "modelConvolucional = Sequential([\n",
    "    Input(input_shape),\n",
    "\n",
    "    Convolution2D(filters=10, kernel_size=(4, 4), strides=1, activation='relu'),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Convolution2D(filters=10, kernel_size=(4, 4), strides=1, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    MaxPooling2D(pool_size=(4, 4)),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(100, activation='tanh'),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Dense(len(CATEGORIAS), activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelConvolucional.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "modelConvolucional.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyConvolutional = modelConvolucional.fit(\n",
    "    train,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    validation_data=validation,\n",
    "    callbacks=[OurCustomCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(historyConvolutional.history['accuracy'], label='train')\n",
    "plt.plot(historyConvolutional.history['val_accuracy'], label='validation')\n",
    "plt.title('Accuracy over train epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolucional modificado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este modelo convolucional utiliza más filtros (32, 64 y 128) para capturar características detalladas en diferentes niveles de abstracción, mientras que los kernels más pequeños de 3x3 permiten una mejor eficiencia en la detección de patrones locales. Las capas de MaxPooling con un tamaño de 2x2 reducen el tamaño de las características progresivamente, ayudando a prevenir el sobreajuste. La activación relu en las capas densas mejora la eficiencia en la propagación de gradientes, y el Dropout más alto (0.5) en las últimas capas evita el sobreajuste, lo que hace al modelo más robusto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelConvolucionalV2 = Sequential([\n",
    "    Input(shape=(150, 150, 3)),\n",
    "\n",
    "    Convolution2D(filters=32, kernel_size=(3, 3), strides=1, activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Convolution2D(filters=64, kernel_size=(3, 3), strides=1, activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Convolution2D(filters=128, kernel_size=(3, 3), strides=1, activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    Dense(len(CATEGORIAS), activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelConvolucionalV2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "modelConvolucionalV2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyConvolutionalV2 = modelConvolucionalV2.fit(\n",
    "    train,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    validation_data=validation,\n",
    "    callbacks=[OurCustomCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(historyConvolutionalV2.history['accuracy'], label='train')\n",
    "plt.plot(historyConvolutionalV2.history['val_accuracy'], label='validation')\n",
    "plt.title('Accuracy over train epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolucional usando convoluciones ya entrenadas de VGG16\n",
    "pretrained_model = VGG16(input_shape=input_shape, include_top=False)\n",
    "pretrained_model.trainable = False\n",
    "\n",
    "modelVGG16 = Sequential([\n",
    "    pretrained_model,\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(100, activation='tanh'),\n",
    "    Dense(100, activation='tanh'),\n",
    "    \n",
    "    Dense(len(CATEGORIAS), activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelVGG16.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "modelVGG16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights_at_epochs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyVGG = modelVGG16.fit(\n",
    "    train,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    validation_data=validation,\n",
    "    callbacks=[OurCustomCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(historyVGG.history['accuracy'], label='train')\n",
    "plt.plot(historyVGG.history['val_accuracy'], label='validation')\n",
    "plt.title('Accuracy over train epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.xlim(-1, 4)\n",
    "# plt.ylim(0.5, 1.0)\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado lo visto en los resultados del modelo VGG16 obtuvimos que:\n",
    "\n",
    "- Epoch 1/5\n",
    "  356s - accuracy: 0.6932 - loss: 0.7998 - val_accuracy: 0.8081 - val_loss: 0.5265\n",
    "- Epoch 2/5\n",
    "  387s - accuracy: 0.8632 - loss: 0.3760 - val_accuracy: 0.8487 - val_loss: 0.4195\n",
    "- Epoch 3/5\n",
    "  380s - accuracy: 0.8873 - loss: 0.3106 - val_accuracy: 0.7666 - val_loss: 0.6069\n",
    "- Epoch 4/5\n",
    "  360s - accuracy: 0.8931 - loss: 0.2758 - val_accuracy: 0.8400 - val_loss: 0.4288\n",
    "- Epoch 5/5\n",
    "  359s - accuracy: 0.8969 - loss: 0.2680 - val_accuracy: 0.8342 - val_loss: 0.4899\n",
    "\n",
    "Elegimos la epoca 2 dado que en las siguientes el modelo overfitea porque el accuracy de validation empieza a caer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelConvolucional.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "modelConvolucional.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights_at_epochs = {}\n",
    "\n",
    "class OurCustomCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        model_weights_at_epochs[epoch] = self.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyCon = modelConvolucional.fit(\n",
    "    train,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    validation_data=validation,\n",
    "    callbacks=[OurCustomCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos el accuracy de ambos conjuntos, tanto train como validation, durante todo el proceso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(historyCon.history['accuracy'], label='train')\n",
    "plt.plot(historyCon.history['val_accuracy'], label='validation')\n",
    "plt.title('Accuracy over train epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elegimos la que consideremos como la mejor epoca y nos quedamos con ese conjunto de pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_EPOCH = 2\n",
    "# modelConvolucional.set_weights(model_weights_at_epochs[BEST_EPOCH])\n",
    "# modelMLP.set_weights(model_weights_at_epochs[BEST_EPOCH])\n",
    "modelVGG16.set_weights(model_weights_at_epochs[BEST_EPOCH])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora analizamos el error de ambos conjuntos para sacar nuestras propias conclusiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = (\n",
    "    ('train', images_reader.flow_from_directory(TRAIN_DIR, **READ_PARAMS, batch_size=-1)),\n",
    "    ('validation', images_reader.flow_from_directory(VALIDATION_DIR, **READ_PARAMS, batch_size=-1)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for dataset_name, dataset in datasets:\n",
    "    print('#' * 25, dataset_name, '#' * 25)\n",
    "\n",
    "    batch_images, batch_labels = next(dataset)\n",
    "    \n",
    "    # super importante: usamos argmax para convertir cosas de este formato:\n",
    "    # [(0, 1, 0), (1, 0, 0), (1, 0, 0), (0, 0, 1)]\n",
    "    # a este formato (donde tenemos el índice de la clase que tiene número más alto):\n",
    "    # [1, 0, 0, 2]\n",
    "    predictions = np.argmax(modelMLP.predict(batch_images), axis=-1)\n",
    "    labels = np.argmax(batch_labels, axis=-1)\n",
    "    \n",
    "    print('Accuracy:', accuracy_score(labels, predictions))\n",
    "\n",
    "    # graficamos la confussion matrix\n",
    "    plt.figure(figsize=(3, 4))\n",
    "        \n",
    "    plt.xticks([0, 1, 2, 3, 4, 5], CATEGORIAS, rotation=45)\n",
    "    plt.yticks([0, 1, 2, 3, 4, 5], CATEGORIAS)\n",
    "    plt.xlabel('Predicted class')\n",
    "    plt.ylabel('True class')\n",
    "\n",
    "    plt.imshow(\n",
    "        confusion_matrix(labels, predictions), \n",
    "        cmap=plt.cm.Blues,\n",
    "        interpolation='nearest',\n",
    "    )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name, dataset in datasets:\n",
    "    print('#' * 25, dataset_name, '#' * 25)\n",
    "\n",
    "    batch_images, batch_labels = next(dataset)\n",
    "    \n",
    "    # super importante: usamos argmax para convertir cosas de este formato:\n",
    "    # [(0, 1, 0), (1, 0, 0), (1, 0, 0), (0, 0, 1)]\n",
    "    # a este formato (donde tenemos el índice de la clase que tiene número más alto):\n",
    "    # [1, 0, 0, 2]\n",
    "    predictions = np.argmax(modelConvolucional.predict(batch_images), axis=-1)\n",
    "    labels = np.argmax(batch_labels, axis=-1)\n",
    "    \n",
    "    print('Accuracy:', accuracy_score(labels, predictions))\n",
    "\n",
    "    # graficamos la confussion matrix\n",
    "    plt.figure(figsize=(3, 4))\n",
    "        \n",
    "    plt.xticks([0, 1, 2, 3, 4, 5], CATEGORIAS, rotation=45)\n",
    "    plt.yticks([0, 1, 2, 3, 4, 5], CATEGORIAS)\n",
    "    plt.xlabel('Predicted class')\n",
    "    plt.ylabel('True class')\n",
    "\n",
    "    plt.imshow(\n",
    "        confusion_matrix(labels, predictions), \n",
    "        cmap=plt.cm.Blues,\n",
    "        interpolation='nearest',\n",
    "    )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name, dataset in datasets:\n",
    "    print('#' * 25, dataset_name, '#' * 25)\n",
    "\n",
    "    batch_images, batch_labels = next(dataset)\n",
    "    \n",
    "    # super importante: usamos argmax para convertir cosas de este formato:\n",
    "    # [(0, 1, 0), (1, 0, 0), (1, 0, 0), (0, 0, 1)]\n",
    "    # a este formato (donde tenemos el índice de la clase que tiene número más alto):\n",
    "    # [1, 0, 0, 2]\n",
    "    predictions = np.argmax(modelVGG16.predict(batch_images), axis=-1)\n",
    "    labels = np.argmax(batch_labels, axis=-1)\n",
    "    \n",
    "    print('Accuracy:', accuracy_score(labels, predictions))\n",
    "\n",
    "    # graficamos la confussion matrix\n",
    "    plt.figure(figsize=(3, 4))\n",
    "        \n",
    "    plt.xticks([0, 1, 2, 3, 4, 5], CATEGORIAS, rotation=45)\n",
    "    plt.yticks([0, 1, 2, 3, 4, 5], CATEGORIAS)\n",
    "    plt.xlabel('Predicted class')\n",
    "    plt.ylabel('True class')\n",
    "\n",
    "    plt.imshow(\n",
    "        confusion_matrix(labels, predictions), \n",
    "        cmap=plt.cm.Blues,\n",
    "        interpolation='nearest',\n",
    "    )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa en los gráficos de confusión que el modelo presenta problemas a la hora de tener que clasificar montañas, prediciendo que son oceanos o glaciares. Por ejemplo, al probar el modelo con la imagen 20058.jpg se muestra la prediccion erronea que planteamos.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ahora probaremos con nuestras propias imágenes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "def show_and_predict(image_path):\n",
    "    image_array = img_to_array(load_img(image_path, target_size=(SIZE, SIZE)))\n",
    "    inputs = np.array([image_array])  # armamos un \"dataset\" con solo esa imagen\n",
    "    predictions = modelVGG16.predict(inputs)\n",
    "    display(Image(image_path, width=500))\n",
    "    print(\"Prediction:\", CATEGORIAS[np.argmax(predictions)])\n",
    "    print(\"Prediction detail:\", predictions)\n",
    "show_and_predict(\"./test/20058.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "CATEGORIA2 = ['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n",
    "SIZE = 150\n",
    "\n",
    "def predict_images_from_directory(directory):\n",
    "    results = [] \n",
    "    images = [] \n",
    "\n",
    "    for image_name in os.listdir(directory):\n",
    "        image_path = os.path.join(directory, image_name)\n",
    "\n",
    "        if image_name.endswith(\".jpg\"):\n",
    "            image_array = img_to_array(load_img(image_path, target_size=(SIZE, SIZE)))\n",
    "            images.append(image_array)\n",
    "\n",
    "    inputs = np.array(images) / 255.0  \n",
    "\n",
    "    predictions = modelVGG16.predict(inputs)\n",
    "\n",
    "    for i, image_name in enumerate(os.listdir(directory)):\n",
    "        if image_name.endswith(\".jpg\"):\n",
    "            predicted_class = CATEGORIA2[np.argmax(predictions[i])]\n",
    "            results.append([image_name, predicted_class])\n",
    "    \n",
    "    df = pd.DataFrame(results, columns=[\"ID\", \"Label\"])\n",
    "    df.to_csv(\"prediccionesVGG.csv\", index=False)\n",
    "    print(\"Predicciones guardadas en 'prediccionesVGG.csv'\")\n",
    "\n",
    "predict_images_from_directory('test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusiones"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
